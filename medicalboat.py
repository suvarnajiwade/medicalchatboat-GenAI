import os
import streamlit as st

from langchain.embeddings import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA

from langchain_community.vectorstores import FAISS
from langchain_core.prompts import PromptTemplate
from langchain_huggingface import HuggingFaceEndpoint

DB_FAISS_PATH = "vectorstore/db_faiss"

@st.cache_resource
def get_vectorstore():
    embedding_model = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')
    db = FAISS.load_local(DB_FAISS_PATH, embedding_model, allow_dangerous_deserialization=True)
    return db

def set_custom_prompt(custom_prompt_template):
    prompt = PromptTemplate(template=custom_prompt_template, input_variables=["context", "question"])
    return prompt

def load_llm(huggingface_repo_id, HF_TOKEN):
    llm = HuggingFaceEndpoint(
        repo_id=huggingface_repo_id,
        temperature=0.5,
        model_kwargs={"token": HF_TOKEN, "max_length": "512"}
    )
    return llm

def main():
    # Add emoji to title for a more engaging experience
    st.title("Ask Chatbot! ğŸ¥ğŸ’¬ğŸ©º")

    if 'messages' not in st.session_state:
        st.session_state.messages = []

    # Display previous messages
    for message in st.session_state.messages:
        st.chat_message(message['role']).markdown(message['content'])

    # Add emoji to the user input prompt
    prompt = st.chat_input("Ask your healthcare-related question here ğŸ©ºğŸ’¡ğŸ’Š")

    if prompt:
        st.chat_message('user').markdown(f"ğŸ‘©â€âš•ï¸ Patient: {prompt}")
        st.session_state.messages.append({'role': 'user', 'content': prompt})

        CUSTOM_PROMPT_TEMPLATE = """
                Use the pieces of information provided in the context to answer user's question.
                If you don't know the answer, just say that you don't know, don't try to make up an answer. 
                Don't provide anything out of the given context.

                Context: {context}
                Question: {question}

                Start the answer directly. No small talk please. ğŸ¥ğŸ’¬ğŸ©º
                """
        
        HUGGINGFACE_REPO_ID = "mistralai/Mistral-7B-Instruct-v0.3"
        HF_TOKEN = os.environ.get("HF_TOKEN")

        try: 
            vectorstore = get_vectorstore()
            if vectorstore is None:
                st.error("Failed to load the vector store ğŸ˜")

            qa_chain = RetrievalQA.from_chain_type(
                llm=load_llm(huggingface_repo_id=HUGGINGFACE_REPO_ID, HF_TOKEN=HF_TOKEN),
                chain_type="stuff",
                retriever=vectorstore.as_retriever(search_kwargs={'k': 3}),
                return_source_documents=True,
                chain_type_kwargs={'prompt': set_custom_prompt(CUSTOM_PROMPT_TEMPLATE)}
            )

            response = qa_chain.invoke({'query': prompt})

            result = response["result"]
            source_documents = response["source_documents"]
            result_to_show = f"""
            **Answer:**
            {result}

            **Source Documents:**
            {str(source_documents)}

            ğŸ¥ **Important Notes:**
            - The information provided here is based on available data.
            - If you need more help, feel free to ask again.
            """
            
            # Display result in a structured and formatted manner
            st.chat_message('assistant').markdown(f"ğŸ‘¨â€âš•ï¸ Doctor: {result_to_show} ğŸ¥ğŸ©ºğŸ’Š")
            st.session_state.messages.append({'role': 'assistant', 'content': result_to_show})

        except Exception as e:
            # Add emoji to error messages
            st.error(f"Error: {str(e)} ğŸ˜ğŸ’Š")

if __name__ == "__main__":
    main()
